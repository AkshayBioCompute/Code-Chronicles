{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f546a2-c951-4919-853f-e1f8f5bb09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install youtube_transcript_api\n",
    "!pip install transformers\n",
    "!pip install openai==0.28\n",
    "!pip install tf-keras\n",
    "!pip install cohere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80511b82-c857-40ac-9184-232d5ac7d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from transformers import pipeline\n",
    "import openai\n",
    "\n",
    "def extract_video_id(youtube_url):\n",
    "    \"\"\"Extracts video ID from a YouTube URL.\"\"\"\n",
    "    try:\n",
    "        return youtube_url.split(\"watch?v=\")[-1]\n",
    "    except IndexError:\n",
    "        raise ValueError(\"Invalid YouTube URL format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "925e11a6-1d25-421b-84e8-ddd49e6eb9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_url = \"https://www.youtube.com/watch?v=0KknQgXzH1g&t=911s\"\n",
    "video_id = extract_video_id(youtube_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e11044c3-1983-4f8c-93e9-f5ac702dde7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error retrieving transcript: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8982aa-ff10-4687-b5f0-4f1524825fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"did you know that men can develop breast cancer although it is much rarer than in women basically less than 1% of all breast cancer can cause occur in men yet approximately something around 2,670 new cases of invasive breast cancer in men are expected in the US in 2005 so today we are discussing about how we can use a breast cancer data by applying the neural network so welcome back to code Chronicles let's start our video so today we are discussing about breast cancer with neural network so first we have to import the libraries import pandas s PD where pandas you know it's a powerful data manipulation and Analysis library and then we use uh numpy as NP which is basically you know about the nump is for numerical computation in pythons which provide supports for arrays matrices and a collection of mathematical functions to operate on these arrays next we are using escalar do model selection and import train test split uh straight for fold so basically uh by this you know the train split so the function splits our data sets into the train tring and test sets Okay and staty fold uh we are using for the close validation method that ensure each fold of the data set maintains the same percentage of samples of each classes okay so basically it is particularly useful for the imbalanced data sets next we are using for the escal and pre-processing label encoder and the Strand standard scalar so basically label and coders are used for class converts categorical labels such as strings into the numerical values uh which is often required for machine learning algorithm okay and standard scalar we are using uh this for the class cheriz features by removing the mean and scaling in the scaling to in unit variance okay it is essentially basically for the many machine learning algorithms especially those that depend on the distance Matrix like uh svm or KNN okay next we are using the m m plot Li uh basically it's a in library that provides function for creating a static interactive and animated visualization okay now we are using the tensor flow caras models uh which is using where we are using sequential and for layers we are using dense Dropout and for Adam uh and for optimizers we are using Adam so basically uh kasas we are using for deep learning uh framework that runs on the top of tensor flow sequential we are using for the it's a linear St of layers where you add layers sequentially and next uh we are using DSE and drop Dropout so Den basically it's a fully connected layer in a neural network where each node is connected to everyone in the previous layer okay and drop Dropout is basically a regularization technique which is used to prevent overfitting by randomly setting a fraction of inputs unit to zero during training this also help us to prevent the model from becoming too dependent on specific neurons okay now at the last we are using Adam so Adam basically adaptive M estimation it is an optimization algorithm that computes adaptive learning rates for each parameter it is widely used for training deep learning models due to its efficiency and ability to handle the sparse gradients okay now we are using the k k Turners uh we are using the random search so basically purpose for K uh Turner is a library to perform the hyper parameter tuning uh where random search tuner is randomly searches through a pre define hyper parameter spaces to find the best model configuration and last we are using the Escalon Matrix which import the confusion Matrix uh basically everyone knows about like who are into the field uh confusion Matrix which summarize the performance of a classification logorithm by compared the predictive levels and the actual levels okay so in the next step we are uh load and pre-process the data so basically we download the breast cancer data sets and prepare it for training we are using uh data set underscore URL so this is the uh URL where the data set is hosted okay so data set in question in the CSV file which contains the information about the breast cancers features and diagnosis okay so then we are using column _ names and then we are reading the data okay we are uh reading the data with the data is equal to pd. read CSV okay so if you want to see the data what we can do is uh we can print data do H parenthesis p is not OB okay so so see you can see the data now we are uh moving to the step three where we can encode the the non- numeric columns as you can see in the data there are so many columns are in the non- numeric and some are numeric okay so what we are doing is we are doing uh the itate through all the columns in the data frames uh first we are uh applying the for Loop okay it basically iterates all over the column names in the data Frame data then what we are doing there is it checks the data type of each column if the column contains n numeric data uh which is a typically a type of object it proceeds with encoding okay next we print encoding non-numeric columns so it prints a message indicating that which column is being encoded and this is very helpful for tracking which columns are being processed okay now at the last uh we are using the label encoders from AAL Lear module to encode the categorical coder okay uh to categorical uh column into numeric values okay so now you can see encoding non numeric column is radius mean as you can see uh there is commas numeric values so all the are non-numeric columns so now uh in step four we are encoding the target variables so basically what uh actually we are using the label encoder over here uh which creates some mapping of each each unique category or label in the input column to a testing uh integer okay now we are using the fit transform uh which is the data is uh taking a diagnosis so the purpose of taking a diagnosis the column data is basically data frame with the newly encoded numeric values okay and in step five we are using the split features and Target so basically features is equal to we are using the data. iog uh where we line the line selects all the columns except the first last line last one the target column as features for the models and uh then we are using the T for Target uh data toog again with minus one so again the line selects the last column as the target variables okay now we are using the standard scalers so the line initiates the standard scalers from the scal and pre-processing that standardize the features by removing the mean and scaling to the unit variance which is where the standard deviation is one which is means the uh the meaning of unit variance is the standard deviation is equal to 1 this is important because many machine learning models perform better or or coverage faster when the data is standardized models like logistic regressions svm and neural networks are sensitive to scales uh of the input features okay and the and we are using the uh features for features is equal to scaler do fit transform features so basically it uh after scaling each features will have a mean of zero and a standard deviation of one okay suppose uh if a feature column originally having a value like 10 15 20 and after scaling it it might be the value of - 1.22 0 1.22 okay so depending on the mean and the standard variation okay now uh in the step six uh we are using the split the data sets into the train and test okay again uh after that we are using the defining the hyper parameter tuning function using K as tuner to Define and tune hyper parameters so basically we are using the sequential Model A sequential model is a linear stack of layers where you can add one layer after after another okay and basically kaas models which are built by layer by layer so it is essential for the sequential model to Ideal uh sequential model is ideal when each layer has a single input and output okay uh when it when it will be happen it will be happen when layers are arranged sequentially okay after that we are using the first dense layer over here okay a fully connected uh layer dense layer layer uh which is added uh and the Dropout layer uh after that so we having a Dropout of 0.1 okay so we are using a Dropout of 0.1 this is the Dropout layer then additional layer we are adding uh for uh dense layer for hyper parameter optimization okay so uh where we are having a Dropout layer of uh 0.1 again and to 0.5 from 0.1 to 0.5 we very we can use this okay now we can add model add dens with the activation of sigmoid okay why we are using sigmoid basically sigmoid activation function outputs a value of 0 to 1 which make it subtle for binary classification suppose we are working on the cancer so we are example we are pred in uh uh cancer or not cancers okay now we compile our model after that we compile our model okay so basically we are using the Adam Optimizer which is used for the Adaptive gradient descent algorithm the learning rate is set using the hyperparameter search so basically we are using the HP do choice for 0.1 0.001 and 0.001 okay this is the learning rate we are choosing and for loss functions uh uh it is a since it is a binary classification problem the loss function we use the binary cross entropy okay and the metrix accuracy is used for the evaluation for metric the model during training okay now in Step eight we perform the hyperparameter tuning and uh where we are using the random search build model Val accuracy uh and uh for the value addition accuracy and maximum trials we are using for the 10 and executions per trial is equal to two uh basically we are using running a multiple executions per trial for allowing for uh allows for averaging the performance to and reducing the variance and then we are create it creating the direct then it will create a directory tuner uh for result the directory where the tuner will store the result of each search okay so you can uh go back to the so you can go back to the directory and you can see the tuner result after running on this and you can see the Jason files okay and try this like this okay now you can give the project name for the breast cancer classification okay now when you run this so basically uh there is a tuner do search we are using so it with having a batch size of 16 we are using in with our 20 appox okay so reloading from the Turner results so you can see the Json file as I saw you now we move to the step nine retrieve for the best parameters so basically we are using uh turn got best parameters the number of Trials we are using one by setting it receives the top trial based on the objective metrix in the case uh validation accuracy that will retrieve the best hyper parameters uh so you can see in the results the best number of units is 24 and the best learning rate is 0.01 okay so now in The Next Step uh we build and train the best model uh now in Step 10 we are build and train the best model okay so we are using tuner do hyper model do build best for the HPS uh during the tuning process so uh now in the this type use the same build model function which you defined earlier like the optimal hyper parameters hyper parameters such as the number of units in each layer the dropout rate and the learning rate okay now we train our model with the history uh is equal to best model. free and with the EPO size 15 and with the on B 16 now you can see the results it will add 50 times I am giving 50 if I giving 40 so it will give the okay I didn't run the code so I have to run everything again for this to run on this 40 and now it will be run on 40 times if you change the now you can see the x is the test features of the test input test set input data and the ground truth so you can see our test accuracy is one after that we perform the cross validation in a step 11 you can see evaluate the model uh so we use loss and accuracy so you can see the test accuracy is one after that we perform uh the cross validation so for cross validations we are using the feature sets and the target labels X and Y okay and now we are using the model builder over here uh basically a function that runs uh returns a a build model the function which is used a create a new instance for the model of each fold and we are splits into the five times Okay so cross validation number of slits is five times that means the data will split into five subsets for training and test and we are using the kold uh kfold okay which creates a objects of for Closs validation which ensures that each fold maintains the same class distribution as the original data sets after that we apply the for Loop over here uh which splits the data sets uh into the N number of five parts and then again it will iterate and return to the train idx to the current fold and then uh for the return to the NP scores uh basically it return return to the list of accuracy scores of each fold as nump AR okay now uh uh now the this function uh basically uh help to build with the best parameters found by the Kash Turner okay now we the score so you can see the cross validation accur accuracy is one plusus 0o and now you can see the visualization of this uh training history you can see the training accuracies one and the validation accuracy is also construct okay after that we generate a confusion Matrix where we find the Precision recall and F1 score and you can see that our model having a good preseason recall and F1 score so guys this is it for the today and I will see you in the next video\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_text = \" \".join([line['text'] for line in transcript])\n",
    "transcript_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f0924a-701e-455a-b041-b8898485688e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast cancer in men are expected in the US in 2005 so today we are discussing about how we the libraries import pandas s PD where pandas you know it's a uh we are using for the close validation method that ensure each fold of the data set scalar okay scalar uh scalar uh which is using where we are using sequential and for layers we are using dense Drop a the model from becoming too dependent on specific neurons okay and drop Dropout is basically efficiency and ability to handle the sparse gradients okay now we are using the k the performance of a classification logorithm by comparing the predictive levels and the actual levels _ names and then we are reading the data okay we are uh reading the data in the data frames uh first we are uh applying the for Loop okay it uh uh uh uh which creates some column to a testing uh integer okay now we are using the fit transform u the unit variance is equal to 1 this is important because many machine models have a lot of are sensitive to scales uh of the input features okay and the uh features uh in the step six uh we are using the defining the hyper parameter uh layer dense layer layer uh which is added uh and the Drop sigmoid okay why we are using sigmoid uh for u the model and then we compile our model okay so basically we are using the Adam Optimizer the model during training okay now in Step eight we perform the hyperparameter tuning and u tuner do search the tuner will store the result of each search okay so you can you see the Json file as I saw you now we move to the step nine retrieve for build and train the best model okay uh now in Step 10 we are build and train the test features of the test input test set input data and the ground truth so you can see the test accuracy is one after that we perform uh the cross validation so you can see and test and we are using the kold uh kfold okay which create uh uh now the this function uh basically uh help to the day and I will see you in the next video. \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the punctuation restoration model\n",
    "punctuator = pipeline(\"text2text-generation\", model=\"t5-small\")\n",
    "\n",
    "def split_text(text, max_length=512):\n",
    "    \"\"\"Split text into chunks of a specified maximum length\"\"\"\n",
    "    # Tokenize and split into chunks that are not longer than the max length\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Add word to the current chunk if it doesn't exceed the max length\n",
    "        if len(\" \".join(chunk + [word])) <= max_length:\n",
    "            chunk.append(word)\n",
    "        else:\n",
    "            # Otherwise, save the current chunk and start a new one\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            chunk = [word]\n",
    "    \n",
    "    # Add any remaining words as a chunk\n",
    "    if chunk:\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Split the transcript text into chunks of manageable size\n",
    "chunks = split_text(transcript_text)\n",
    "\n",
    "# Restore punctuation for each chunk and join the results\n",
    "transcript_with_punctuation = \"\"\n",
    "for chunk in chunks:\n",
    "    result = punctuator(chunk)\n",
    "    transcript_with_punctuation += result[0]['generated_text'] + \" \"\n",
    "\n",
    "# Print the final result with restored punctuation\n",
    "print(transcript_with_punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375b3bc-8bd8-4821-ac41-4ecd121a936f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73f6c329-c7b3-43c2-8972-83a15d98f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "cohere_api_key = \"Apikey\"  # Replace with your actual Cohere API key\n",
    "co = cohere.Client(cohere_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a566dc93-2968-4c91-a7a3-61630a73ad2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Breast cancer is a type of cancer that occurs in the breast tissue. It is characterized by the abnormal growth of cells, which can form a tumor.  Breast cancer is the most common type of cancer among women, but it can also occur in men. \n",
      "\n",
      "In the provided text, the speaker is discussing a computational method for classifying breast cancer cases using artificial intelligence. They do not provide an explanation of the biology or causes of breast cancer in men or women. \n"
     ]
    }
   ],
   "source": [
    "# Define the question prompt\n",
    "question_prompt = f'Answer the following question based on the provided text:\\n\\nText: \"{transcript_with_punctuation} What is breast cancer?\"'\n",
    "\n",
    "# Generate a response using the default model\n",
    "response = co.generate(\n",
    "    prompt=question_prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Display the response\n",
    "print(response.generations[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a2c28-297b-459f-8bd5-31fa01fc6ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de5173-a168-4f40-89ee-26f9587d3166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ee8ab-6833-493c-a15c-4b552b4d1850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
